{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNma1Mf9tkZvt49qKY1MC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuruurajD/AI4ICPS-Learning/blob/main/LLM_Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import re\n",
        "\n",
        "##### You may comment this section to see verbose -- but you must un-comment this before final submission. ######\n",
        "transformers.logging.set_verbosity_error()\n",
        "transformers.utils.logging.disable_progress_bar()\n",
        "#################################################################################################################\n",
        "\n",
        "def clean_decoded_output(decoded_output):\n",
        "    # clean the output from LLM and return\n",
        "    cleaned = decoded_output.strip().upper()\n",
        "    return cleaned\n",
        "\n",
        "def llm_function(model,tokenizer,a,b):\n",
        "    '''\n",
        "    The steps are given for your reference:\n",
        "\n",
        "    1. Properly formulate the prompt as per the question - which should output either 'YES' or 'NO'. The output must always be upper-case. You may post-process to get the desired output.\n",
        "    2. Tokenize the prompt\n",
        "    3. Pass the tokenized prompt to the model and generate output. You can use default hyper-parameters for the model.\n",
        "    4. Decode the model output.\n",
        "    5. Clean output and return.\n",
        "\n",
        "    Note: The model (Flan-T5-XL) and tokenizer is already initialized. Do not modify that section.\n",
        "    '''\n",
        "    prompt = f\"Does '{b}' correctly answer the question '{a}'? Answer only YES or NO.\"\n",
        "\n",
        "    # Tokenizing prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generating model output\n",
        "    outputs = model.generate(input_ids)\n",
        "\n",
        "    # Decoding the output\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Cleaning output\n",
        "    cleaned_output = clean_decoded_output(decoded_output)\n",
        "    return cleaned_output"
      ],
      "metadata": {
        "id": "B_c9dkYyhEPV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = 'Who is Rabindranath Tagore?'\n",
        "p2 = 'He is a famous poet, writer, playwright, composer, philosopher, social reformer, and painter of the Bengal Renaissance'\n",
        "input_data_one = p1.strip()\n",
        "input_data_two = p2.strip()\n",
        "\n",
        "    ##################### Loading Model and Tokenizer ########################\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
        "    ##########################################################################\n",
        "\n",
        "\"\"\"  Call to function that will perform the computation. \"\"\"\n",
        "\n",
        "a = input_data_one\n",
        "b = input_data_two\n",
        "\n",
        "torch.manual_seed(42)\n",
        "out = llm_function(model,tokenizer,a,b)\n",
        "print(out.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pB3anwFhw7l",
        "outputId": "64e64a4d-5b51-4e41-da50-046950696344"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YES\n"
          ]
        }
      ]
    }
  ]
}